# Модуль DeepSeek (LLM) в проекте

## Что делает DeepSeek

1. **Мемо по блокам** (`memo_engine.py`)  
   Для каждого типа данных (R, O, keyness, эссенциализация, сети, embeddings) в LLM отправляется сводка (counts, нормировки, топ-значения, до 5 примеров). Ответ: структурированный JSON с полями `memo_text`, `hypotheses_to_check`, `pitfalls`. Результаты сохраняются в `output/llm_memos/{representation,situation,keyness,...}.json` и подставляются в HTML-отчёт как «расширенная интерпретация».

2. **Межтабличный синтез** (`synthesis.py`)  
   После расчёта производных индексов (OI, AS, ED, EPS) в LLM отправляются индексы, топ keyness и плотность сети. Ответ: структурная модель ориентализации, согласованность индексов, 5 исследовательских направлений, 3 гипотезы, ограничения, краткий синтетический текст. Сохраняется в `output/llm_memos/synthesis.json` и выводится в разделе «Структурная модель» HTML-отчёта.

3. **Единый обзор для PDF** (`src/deepseek_analysis.py`)  
   При запуске с `--run-llm` вызывается `run_analysis()`: в LLM передаются нормировки по этносам/R/O, топ keyness, эссенциализация, сводка по сетям, доли uncertain/unknown, шум. Ответ: `observations`, `probable_patterns`, `limitations`. Сохраняется в `output/llm_analysis.json` и используется в итоговом PDF (`final_report.pdf`).

4. **Реактивная аналитика** (`api/app.py`)  
   Если запущен локальный API (`python -m api.app`), в HTML-отчёте кнопка «Пересчитать аналитическую интерпретацию» отправляет текущую сводку блока в DeepSeek и подставляет новый memo без перезапуска пайплайна. Кэш по hash сводки в `output/llm_cache/`.

## Как использовать по максимуму

- **API-ключ**: задайте `DEEPSEEK_API_KEY` в окружении или в файле `.env` в корне проекта.
- **Включить LLM при сборке отчёта**:  
  `python main.py --run-llm`  
  или  
  `python main.py --full`  
  Тогда выполняются мемо по блокам, синтез и обзор для PDF.
- **Реактивная аналитика**:  
  1) `python -m api.app` (порт 5000);  
  2) откройте `output/report.html`;  
  3) в отчёте укажите `api_base` (по умолчанию `http://127.0.0.1:5000`) при сборке или через переменную `DASHBOARD_API_BASE`;  
  4) нажимайте «Пересчитать аналитическую интерпретацию» для нужного блока — запрос уйдёт в DeepSeek, ответ подставится в блок.
- **Системный промпт**: в `llm/system_prompt.txt` задаётся роль (исследователь травелогов, ориентализм, Сибирь) и формат ответа. Редактирование файла меняет тон и акценты всех ответов LLM.
- **База знаний (теоретическая рамка)**: в `resources/knowledge/domain_briefing.txt` хранится краткий брифинг по ориентализму (Саид), травелогам, русской империи и Сибири, эссенциализации. Он подставляется в системный промпт (плейсхолдер `{{DOMAIN_KNOWLEDGE}}`), чтобы LLM интерпретировал данные в духе специалиста по травелогам и постколониальным/имперским исследованиям. Редактирование `domain_briefing.txt` или добавление файлов в `resources/knowledge/` позволяет углубить теоретическую интерпретацию; при отсутствии файла используется краткая подсказка.
- **Логи**: вызовы к API логируются в `output/logs/llm_calls.log` (длины запросов/ответов, ошибки).

## Файлы

| Файл | Назначение |
|------|------------|
| `llm/deepseek_client.py` | Загрузка промпта, вызов API DeepSeek (OpenAI-совместимый), логирование |
| `llm/knowledge_loader.py` | Загрузка базы знаний (`domain_briefing.txt`) и подстановка в системный промпт |
| `llm/memo_engine.py` | Генерация и загрузка мемо по блокам (representation, situation, keyness, …) |
| `llm/synthesis.py` | Синтез по индексам OI/AS/ED/EPS, вывод в synthesis.json |
| `llm/system_prompt.txt` | Системный промпт (с плейсхолдером для базы знаний) |
| `resources/knowledge/domain_briefing.txt` | Теоретическая рамка: ориентализм, травелоги, Сибирь в имперском дискурсе |
| `src/deepseek_analysis.py` | Единый обзор для PDF (run_analysis → llm_analysis.json) |
| `api/app.py` | Flask API для реактивного пересчёта memo по кнопке в отчёте |

## Ограничения

- Без `DEEPSEEK_API_KEY` все вызовы возвращают заглушку «LLM disabled»; используются только rule-based мемо и синтез.
- В LLM передаются только агрегаты и короткие примеры; полные тексты не отправляются.
- Модель по умолчанию: `deepseek-chat`; смена модели требует правки в `deepseek_client.py` и при необходимости в `src/deepseek_analysis.py`.
